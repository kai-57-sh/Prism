#!/usr/bin/env python
"""
Qwen LLM é›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯• ModelScope Qwen3-235B-A22B-Instruct-2507 è¯­è¨€æ¨¡å‹
"""

import os
import sys
import json
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

def test_basic_qwen_call():
    """æµ‹è¯•åŸºæœ¬çš„ Qwen æ¨¡å‹è°ƒç”¨"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: åŸºæœ¬ Qwen æ¨¡å‹è°ƒç”¨")
    print("="*60)

    from langchain_openai import ChatOpenAI
    from langchain.schema import HumanMessage
    from src.config.settings import settings

    llm = ChatOpenAI(
        model=settings.qwen_model,
        api_key=settings.modelscope_api_key,
        base_url=settings.modelscope_base_url,
        temperature=0.0,
    )

    messages = [HumanMessage(content="ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚")]

    print(f"ğŸ“ å‘é€æ¶ˆæ¯: {messages[0].content}")
    response = llm.invoke(messages)

    print(f"âœ… å“åº”æˆåŠŸ")
    print(f"ğŸ“„ å“åº”å†…å®¹: {response.content}")
    print(f"ğŸ“Š å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")

    assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
    assert len(response.content) > 0, "å“åº”é•¿åº¦ä¸º0"

    return True


def test_qwen_with_system_message():
    """æµ‹è¯•å¸¦ç³»ç»Ÿæ¶ˆæ¯çš„ Qwen è°ƒç”¨"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: å¸¦ç³»ç»Ÿæ¶ˆæ¯çš„ Qwen è°ƒç”¨")
    print("="*60)

    from langchain_openai import ChatOpenAI
    from langchain.schema import HumanMessage, SystemMessage
    from src.config.settings import settings

    llm = ChatOpenAI(
        model=settings.qwen_model,
        api_key=settings.modelscope_api_key,
        base_url=settings.modelscope_base_url,
        temperature=0.0,
    )

    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—è§†é¢‘ç”ŸæˆåŠ©æ‰‹ã€‚"),
        HumanMessage(content="è¯·ç”Ÿæˆä¸€ä¸ªå…³äºå¤±çœ çš„ç®€çŸ­æè¿°ã€‚")
    ]

    print(f"ğŸ“ ç³»ç»Ÿæ¶ˆæ¯: {messages[0].content}")
    print(f"ğŸ“ ç”¨æˆ·æ¶ˆæ¯: {messages[1].content}")

    response = llm.invoke(messages)

    print(f"âœ… å“åº”æˆåŠŸ")
    print(f"ğŸ“„ å“åº”å†…å®¹: {response.content}")

    assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
    assert "å¤±çœ " in response.content or "ç¡çœ " in response.content, "å“åº”å†…å®¹ä¸ä¸»é¢˜ä¸ç¬¦"

    return True


def test_qwen_json_output():
    """æµ‹è¯• Qwen JSON è¾“å‡º"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: Qwen JSON è¾“å‡º")
    print("="*60)

    from langchain_openai import ChatOpenAI
    from langchain.schema import HumanMessage
    from src.config.settings import settings

    llm = ChatOpenAI(
        model=settings.qwen_model,
        api_key=settings.modelscope_api_key,
        base_url=settings.modelscope_base_url,
        temperature=0.0,
    )

    prompt = """è¯·ä»¥ JSON æ ¼å¼è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼š
{
    "topic": "å¤±çœ ",
    "intent": "mood_video",
    "emotion": ["ç„¦è™‘", "å¹³é™"]
}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    messages = [HumanMessage(content=prompt)]

    print(f"ğŸ“ å‘é€æç¤º: {prompt[:50]}...")

    response = llm.invoke(messages)

    print(f"âœ… å“åº”æˆåŠŸ")
    print(f"ğŸ“„ å“åº”å†…å®¹: {response.content}")

    # Try to parse JSON
    try:
        parsed = json.loads(response.content)
        print(f"ğŸ“Š è§£æåçš„ JSON: {json.dumps(parsed, ensure_ascii=False, indent=2)}")
        assert "topic" in parsed, "JSON ä¸­ç¼ºå°‘ 'topic' å­—æ®µ"
        assert "intent" in parsed, "JSON ä¸­ç¼ºå°‘ 'intent' å­—æ®µ"
        print(f"âœ… JSON è§£ææˆåŠŸ")
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}")
        raise

    return True


def test_qwen_chinese_understanding():
    """æµ‹è¯• Qwen ä¸­æ–‡ç†è§£èƒ½åŠ›"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: Qwen ä¸­æ–‡ç†è§£èƒ½åŠ›")
    print("="*60)

    from langchain_openai import ChatOpenAI
    from langchain.schema import HumanMessage
    from src.config.settings import settings

    llm = ChatOpenAI(
        model=settings.qwen_model,
        api_key=settings.modelscope_api_key,
        base_url=settings.modelscope_base_url,
        temperature=0.0,
    )

    messages = [
        HumanMessage(content="è¯·å°†'ç„¦è™‘'ã€'å¤±çœ 'ã€'æ”¾æ¾'è¿™ä¸‰ä¸ªè¯æŒ‰æƒ…ç»ªä»è´Ÿé¢åˆ°æ­£é¢æ’åºã€‚")
    ]

    print(f"ğŸ“ å‘é€æ¶ˆæ¯: {messages[0].content}")

    response = llm.invoke(messages)

    print(f"âœ… å“åº”æˆåŠŸ")
    print(f"ğŸ“„ å“åº”å†…å®¹: {response.content}")

    assert response.content, "å“åº”å†…å®¹ä¸ºç©º"

    return True


def test_qwen_structured_extraction():
    """æµ‹è¯• Qwen ç»“æ„åŒ–ä¿¡æ¯æå–"""
    print("\n" + "="*60)
    print("æµ‹è¯• 5: Qwen ç»“æ„åŒ–ä¿¡æ¯æå–")
    print("="*60)

    from langchain_openai import ChatOpenAI
    from langchain.schema import HumanMessage
    from src.config.settings import settings

    llm = ChatOpenAI(
        model=settings.qwen_model,
        api_key=settings.modelscope_api_key,
        base_url=settings.modelscope_base_url,
        temperature=0.0,
    )

    user_input = "æˆ‘æƒ³è¦ä¸€ä¸ªå…³äºå¤±çœ çš„èˆ’ç¼“è§†é¢‘ï¼Œæ—¶é•¿ 10 ç§’ï¼Œæš–è‰²è°ƒ"

    prompt = f"""ä»ä»¥ä¸‹ç”¨æˆ·è¾“å…¥ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼š

ç”¨æˆ·è¾“å…¥: {user_input}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
    "topic": "ä¸»é¢˜",
    "intent": "æ„å›¾",
    "duration_preference_s": æ—¶é•¿ï¼ˆæ•°å­—ï¼‰ï¼Œ
    "color_tone": "è‰²è°ƒ"
}}

åªè¿”å› JSONã€‚"""

    messages = [HumanMessage(content=prompt)]

    print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
    print(f"ğŸ“ å‘é€æç¤º: {prompt[:80]}...")

    response = llm.invoke(messages)

    print(f"âœ… å“åº”æˆåŠŸ")
    print(f"ğŸ“„ å“åº”å†…å®¹: {response.content}")

    try:
        parsed = json.loads(response.content)
        print(f"ğŸ“Š è§£æåçš„ JSON: {json.dumps(parsed, ensure_ascii=False, indent=2)}")

        # Validate extracted information
        assert "topic" in parsed, "JSON ä¸­ç¼ºå°‘ 'topic' å­—æ®µ"
        assert parsed["topic"] == "å¤±çœ " or "å¤±çœ " in parsed["topic"], f"ä¸»é¢˜æå–é”™è¯¯: {parsed['topic']}"

        if "duration_preference_s" in parsed:
            print(f"âœ… ä¸»é¢˜: {parsed['topic']}")
            print(f"âœ… æ—¶é•¿: {parsed.get('duration_preference_s', 'N/A')} ç§’")
            print(f"âœ… è‰²è°ƒ: {parsed.get('color_tone', 'N/A')}")

        print(f"âœ… ä¿¡æ¯æå–æˆåŠŸ")
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}")
        print(f"   åŸå§‹å“åº”: {response.content}")
        raise

    return True


def test_llm_orchestrator_parse_ir():
    """æµ‹è¯• LLM Orchestrator çš„ IR è§£æåŠŸèƒ½"""
    print("\n" + "="*60)
    print("æµ‹è¯• 6: LLM Orchestrator - IR è§£æ")
    print("="*60)

    from src.core.llm_orchestrator import LLMOrchestrator

    orchestrator = LLMOrchestrator()

    # Test simple input
    user_input = "æˆ‘æƒ³è¦ä¸€ä¸ªå…³äºå¤±çœ çš„èˆ’ç¼“è§†é¢‘"

    print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")

    ir = orchestrator.parse_ir(user_input, quality_mode="balanced")

    print(f"âœ… IR è§£ææˆåŠŸ")
    print(f"ğŸ“Š IR å†…å®¹:")
    print(f"   - ä¸»é¢˜: {ir.topic}")
    print(f"   - æ„å›¾: {ir.intent}")
    print(f"   - æ—¶é•¿: {ir.duration_preference_s} ç§’")
    print(f"   - è´¨é‡æ¨¡å¼: {ir.quality_mode}")
    print(f"   - é£æ ¼: {ir.style}")
    print(f"   - åœºæ™¯: {ir.scene}")
    print(f"   - æƒ…ç»ªæ›²çº¿: {ir.emotion_curve}")

    assert ir.topic, "IR ä¸­ç¼ºå°‘ä¸»é¢˜"
    assert ir.intent, "IR ä¸­ç¼ºå°‘æ„å›¾"
    assert ir.duration_preference_s > 0, "IR ä¸­æ—¶é•¿åº”è¯¥å¤§äº0"
    assert ir.quality_mode == "balanced", "IR ä¸­è´¨é‡æ¨¡å¼ä¸æ­£ç¡®"

    return True


def test_llm_orchestrator_detailed_ir():
    """æµ‹è¯• LLM Orchestrator çš„è¯¦ç»† IR è§£æ"""
    print("\n" + "="*60)
    print("æµ‹è¯• 7: LLM Orchestrator - è¯¦ç»† IR è§£æ")
    print("="*60)

    from src.core.llm_orchestrator import LLMOrchestrator

    orchestrator = LLMOrchestrator()

    user_input = """æˆ‘æƒ³è¦ä¸€ä¸ª10ç§’çš„è§†é¢‘ï¼Œä¸»é¢˜æ˜¯å¤±çœ æ²»ç–—ã€‚
é£æ ¼è¦èˆ’ç¼“ã€æš–è‰²è°ƒã€‚åœºæ™¯è®¾å®šåœ¨å§å®¤ã€‚
æƒ…ç»ªä»ç„¦è™‘é€æ¸è½¬ä¸ºå¹³é™ã€‚ä¸éœ€è¦å­—å¹•ã€‚"""

    print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")

    ir = orchestrator.parse_ir(user_input, quality_mode="high")

    print(f"âœ… IR è§£ææˆåŠŸ")
    print(f"ğŸ“Š IR å†…å®¹:")
    print(f"   - ä¸»é¢˜: {ir.topic}")
    print(f"   - æ„å›¾: {ir.intent}")
    print(f"   - æ—¶é•¿: {ir.duration_preference_s} ç§’")
    print(f"   - è´¨é‡æ¨¡å¼: {ir.quality_mode}")
    print(f"   - é£æ ¼: {ir.style}")
    print(f"   - åœºæ™¯: {ir.scene}")
    print(f"   - æƒ…ç»ªæ›²çº¿: {ir.emotion_curve}")
    print(f"   - å­—å¹•ç­–ç•¥: {ir.subtitle_policy}")

    # Accept both Chinese and English topic (LLM may translate)
    assert ir.topic in ["å¤±çœ ", "insomnia", "å¤±çœ æ²»ç–—"] or "å¤±çœ " in ir.topic or "insomnia" in ir.topic.lower(), \
        f"ä¸»é¢˜åº”è¯¥æ˜¯'å¤±çœ 'ç›¸å…³ï¼Œå®é™…æ˜¯: {ir.topic}"
    assert ir.style, "IR ä¸­ç¼ºå°‘é£æ ¼ä¿¡æ¯"
    assert ir.scene, "IR ä¸­ç¼ºå°‘åœºæ™¯ä¿¡æ¯"
    assert len(ir.emotion_curve) >= 2, "æƒ…ç»ªæ›²çº¿åº”è¯¥è‡³å°‘æœ‰2ä¸ªå…ƒç´ "
    assert ir.duration_preference_s == 10, "æ—¶é•¿åº”è¯¥æ˜¯10ç§’"

    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "â•”" + "="*58 + "â•—")
    print("â•‘" + " "*10 + "Qwen LLM é›†æˆæµ‹è¯•å¥—ä»¶" + " "*20 + "â•‘")
    print("â•š" + "="*58 + "â•")

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("\nğŸ” æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    if not os.getenv("MODELSCOPE_API_KEY"):
        print("âŒ MODELSCOPE_API_KEY æœªè®¾ç½®")
        print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® MODELSCOPE_API_KEY")
        return False

    if not os.getenv("QWEN_MODEL"):
        print("âŒ QWEN_MODEL æœªè®¾ç½®")
        print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® QWEN_MODEL")
        return False

    print(f"âœ… MODELSCOPE_API_KEY: {os.getenv('MODELSCOPE_API_KEY')[:20]}...")
    print(f"âœ… QWEN_MODEL: {os.getenv('QWEN_MODEL')}")
    print(f"âœ… MODELSCOPE_BASE_URL: {os.getenv('MODELSCOPE_BASE_URL')}")

    # è¿è¡Œæµ‹è¯•
    tests = [
        ("åŸºæœ¬è°ƒç”¨", test_basic_qwen_call),
        ("ç³»ç»Ÿæ¶ˆæ¯", test_qwen_with_system_message),
        ("JSON è¾“å‡º", test_qwen_json_output),
        ("ä¸­æ–‡ç†è§£", test_qwen_chinese_understanding),
        ("ä¿¡æ¯æå–", test_qwen_structured_extraction),
        ("IR è§£æ", test_llm_orchestrator_parse_ir),
        ("è¯¦ç»† IR è§£æ", test_llm_orchestrator_detailed_ir),
    ]

    passed = 0
    failed = 0

    for name, test_func in tests:
        try:
            print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {name}")
            test_func()
            passed += 1
            print(f"âœ… æµ‹è¯• '{name}' é€šè¿‡")
        except Exception as e:
            failed += 1
            print(f"âŒ æµ‹è¯• '{name}' å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

    # æµ‹è¯•æ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"âœ… é€šè¿‡: {passed}/{len(tests)}")
    print(f"âŒ å¤±è´¥: {failed}/{len(tests)}")
    print(f"ğŸ“Š æˆåŠŸç‡: {passed/len(tests)*100:.1f}%")

    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Qwen LLM é›†æˆæ­£å¸¸å·¥ä½œã€‚")
        return True
    else:
        print(f"\nâš ï¸  æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
